"""
This type stub file was generated by pyright.
"""

import httpx
from typing import Dict, Iterable, List, Optional, Union
from typing_extensions import Literal
from ...._types import Body, Headers, NotGiven, Query
from ...._utils import async_with_sts_token, with_sts_token
from ...._compat import cached_property
from ...._resource import AsyncAPIResource, SyncAPIResource
from ....types.chat import completion_create_params
from ....common._parsing import ResponseFormatT
from ....common.streaming.chat import AsyncChatCompletionStreamManager, ChatCompletionStreamManager
from ....types.chat.parsed_chat_completion import ParsedChatCompletion
from ....types.chat.chat_completion_tool_param import ChatCompletionToolParam
from ....types.chat.chat_completion_message_param import ChatCompletionMessageParam
from ....types.chat.chat_completion_stream_options_param import ChatCompletionStreamOptionsParam
from ....types.chat.chat_completion_tool_choice_option_param import ChatCompletionToolChoiceOptionParam
from ....types.shared.reasoning_effort import ReasoningEffort

__all__ = ["Completions", "AsyncCompletions"]
class Completions(SyncAPIResource):
    @cached_property
    def with_raw_response(self) -> CompletionsWithRawResponse:
        """
        This property can be used as a prefix for any HTTP method call to return the
        the raw response object instead of the parsed content.
        """
        ...
    
    @cached_property
    def with_streaming_response(self) -> CompletionsWithStreamingResponse:
        """
        An alternative to `.with_raw_response` that doesn't eagerly read the response body.
        """
        ...
    
    @with_sts_token
    def parse(self, *, messages: Iterable[ChatCompletionMessageParam], model: str, response_format: type[ResponseFormatT] | NotGiven = ..., frequency_penalty: Optional[float] | NotGiven = ..., logit_bias: Optional[Dict[str, int]] | NotGiven = ..., logprobs: Optional[bool] | NotGiven = ..., max_tokens: Optional[int] | NotGiven = ..., n: Optional[int] | NotGiven = ..., parallel_tool_calls: bool | NotGiven = ..., presence_penalty: Optional[float] | NotGiven = ..., service_tier: Optional[Literal["auto", "default"]] | NotGiven = ..., stop: Union[Optional[str], List[str], None] | NotGiven = ..., stream_options: Optional[ChatCompletionStreamOptionsParam] | NotGiven = ..., temperature: Optional[float] | NotGiven = ..., tool_choice: ChatCompletionToolChoiceOptionParam | NotGiven = ..., tools: Iterable[ChatCompletionToolParam] | NotGiven = ..., top_logprobs: Optional[int] | NotGiven = ..., top_p: Optional[float] | NotGiven = ..., user: str | NotGiven = ..., reasoning_effort: Optional[ReasoningEffort] | NotGiven = ..., extra_headers: Headers | None = ..., extra_query: Query | None = ..., extra_body: Body | None = ..., timeout: float | httpx.Timeout | None | NotGiven = ...) -> ParsedChatCompletion[ResponseFormatT]:
        ...
    
    def stream(self, *, messages: Iterable[ChatCompletionMessageParam], model: str, response_format: completion_create_params.ResponseFormat | type[ResponseFormatT] | NotGiven = ..., frequency_penalty: Optional[float] | NotGiven = ..., logit_bias: Optional[Dict[str, int]] | NotGiven = ..., logprobs: Optional[bool] | NotGiven = ..., max_tokens: Optional[int] | NotGiven = ..., n: Optional[int] | NotGiven = ..., parallel_tool_calls: bool | NotGiven = ..., presence_penalty: Optional[float] | NotGiven = ..., service_tier: Optional[Literal["auto", "default"]] | NotGiven = ..., stop: Union[Optional[str], List[str], None] | NotGiven = ..., stream_options: Optional[ChatCompletionStreamOptionsParam] | NotGiven = ..., temperature: Optional[float] | NotGiven = ..., tool_choice: ChatCompletionToolChoiceOptionParam | NotGiven = ..., tools: Iterable[ChatCompletionToolParam] | NotGiven = ..., top_logprobs: Optional[int] | NotGiven = ..., top_p: Optional[float] | NotGiven = ..., user: str | NotGiven = ..., reasoning_effort: Optional[ReasoningEffort] | NotGiven = ..., extra_headers: Headers | None = ..., extra_query: Query | None = ..., extra_body: Body | None = ..., timeout: float | httpx.Timeout | None | NotGiven = ...) -> ChatCompletionStreamManager[ResponseFormatT]:
        ...
    


class AsyncCompletions(AsyncAPIResource):
    @cached_property
    def with_raw_response(self) -> AsyncCompletionsWithRawResponse:
        """
        This property can be used as a prefix for any HTTP method call to return the
        the raw response object instead of the parsed content.
        """
        ...
    
    @cached_property
    def with_streaming_response(self) -> AsyncCompletionsWithStreamingResponse:
        """
        An alternative to `.with_raw_response` that doesn't eagerly read the response body.
        """
        ...
    
    @async_with_sts_token
    async def parse(self, *, messages: Iterable[ChatCompletionMessageParam], model: str, response_format: type[ResponseFormatT] | NotGiven = ..., frequency_penalty: Optional[float] | NotGiven = ..., logit_bias: Optional[Dict[str, int]] | NotGiven = ..., logprobs: Optional[bool] | NotGiven = ..., max_tokens: Optional[int] | NotGiven = ..., n: Optional[int] | NotGiven = ..., parallel_tool_calls: bool | NotGiven = ..., presence_penalty: Optional[float] | NotGiven = ..., service_tier: Optional[Literal["auto", "default"]] | NotGiven = ..., stop: Union[Optional[str], List[str], None] | NotGiven = ..., stream_options: Optional[ChatCompletionStreamOptionsParam] | NotGiven = ..., temperature: Optional[float] | NotGiven = ..., tool_choice: ChatCompletionToolChoiceOptionParam | NotGiven = ..., tools: Iterable[ChatCompletionToolParam] | NotGiven = ..., top_logprobs: Optional[int] | NotGiven = ..., top_p: Optional[float] | NotGiven = ..., user: str | NotGiven = ..., reasoning_effort: Optional[ReasoningEffort] | NotGiven = ..., extra_headers: Headers | None = ..., extra_query: Query | None = ..., extra_body: Body | None = ..., timeout: float | httpx.Timeout | None | NotGiven = ...) -> ParsedChatCompletion[ResponseFormatT]:
        ...
    
    def stream(self, *, messages: Iterable[ChatCompletionMessageParam], model: str, response_format: completion_create_params.ResponseFormat | type[ResponseFormatT] | NotGiven = ..., frequency_penalty: Optional[float] | NotGiven = ..., logit_bias: Optional[Dict[str, int]] | NotGiven = ..., logprobs: Optional[bool] | NotGiven = ..., max_tokens: Optional[int] | NotGiven = ..., n: Optional[int] | NotGiven = ..., parallel_tool_calls: bool | NotGiven = ..., presence_penalty: Optional[float] | NotGiven = ..., service_tier: Optional[Literal["auto", "default"]] | NotGiven = ..., stop: Union[Optional[str], List[str], None] | NotGiven = ..., stream_options: Optional[ChatCompletionStreamOptionsParam] | NotGiven = ..., temperature: Optional[float] | NotGiven = ..., tool_choice: ChatCompletionToolChoiceOptionParam | NotGiven = ..., tools: Iterable[ChatCompletionToolParam] | NotGiven = ..., top_logprobs: Optional[int] | NotGiven = ..., top_p: Optional[float] | NotGiven = ..., user: str | NotGiven = ..., reasoning_effort: Optional[ReasoningEffort] | NotGiven = ..., extra_headers: Headers | None = ..., extra_query: Query | None = ..., extra_body: Body | None = ..., timeout: float | httpx.Timeout | None | NotGiven = ...) -> AsyncChatCompletionStreamManager[ResponseFormatT]:
        ...
    


class CompletionsWithRawResponse:
    def __init__(self, completions: Completions) -> None:
        ...
    


class AsyncCompletionsWithRawResponse:
    def __init__(self, completions: AsyncCompletions) -> None:
        ...
    


class CompletionsWithStreamingResponse:
    def __init__(self, completions: Completions) -> None:
        ...
    


class AsyncCompletionsWithStreamingResponse:
    def __init__(self, completions: AsyncCompletions) -> None:
        ...
    


